import streamlit as st
import pandas as pd
import altair as alt
from kafka import KafkaProducer
import json
import time
import os
import uuid
import psycopg2

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Kafka
KAFKA_CONFIG = {
    "bootstrap_servers": os.getenv("KAFKA_BROKERS", "kafka:9092"),
    "topic": os.getenv("KAFKA_TOPIC", "transactions")
}

POSTGRES_CONFIG = {
    "host": os.getenv("POSTGRES_HOST", "postgres"),
    "port": os.getenv("POSTGRES_PORT", "5432"),
    "dbname": os.getenv("POSTGRES_DB", "fraud_db"),
    "user": os.getenv("POSTGRES_USER", "fraud_user"),
    "password": os.getenv("POSTGRES_PASSWORD", "fraud_pass")
}

def load_file(uploaded_file):
    """–ó–∞–≥—Ä—É–∑–∫–∞ CSV —Ñ–∞–π–ª–∞ –≤ DataFrame"""
    try:
        return pd.read_csv(uploaded_file)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {str(e)}")
        return None

def send_to_kafka(df, topic, bootstrap_servers):
    """–û—Ç–ø—Ä–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ Kafka —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º ID —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"""
    try:
        producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode("utf-8"),
            security_protocol="PLAINTEXT"
        )
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ID –¥–ª—è –≤—Å–µ—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
        df['transaction_id'] = [str(uuid.uuid4()) for _ in range(len(df))]
        
        progress_bar = st.progress(0)
        total_rows = len(df)
        
        for idx, row in df.iterrows():
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤–º–µ—Å—Ç–µ —Å ID
            producer.send(
                topic, 
                value={
                    "transaction_id": row['transaction_id'],
                    "data": row.drop('transaction_id').to_dict()
                }
            )
            progress_bar.progress((idx + 1) / total_rows)
            time.sleep(0.01)
            
        producer.flush()
     
        return True
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        return False

def fetch_recent_frauds(conn, limit=10):
    query = """
        SELECT transaction_id, score, fraud_flag, created_at
        FROM fraud_scores
        WHERE fraud_flag = 1
        ORDER BY created_at DESC
        LIMIT %s
    """
    return pd.read_sql_query(query, conn, params=(limit,))


def fetch_recent_scores(conn, limit=100):
    query = """
        SELECT score
        FROM fraud_scores
        ORDER BY created_at DESC
        LIMIT %s
    """
    return pd.read_sql_query(query, conn, params=(limit,))


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
if "uploaded_files" not in st.session_state:
    st.session_state.uploaded_files = {}

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.title("üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ Kafka")

# –ë–ª–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤
uploaded_file = st.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Ñ–∞–π–ª —Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏",
    type=["csv"]
)

if uploaded_file and uploaded_file.name not in st.session_state.uploaded_files:
    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    st.session_state.uploaded_files[uploaded_file.name] = {
        "status": "–ó–∞–≥—Ä—É–∂–µ–Ω",
        "df": load_file(uploaded_file)
    }
    st.success(f"–§–∞–π–ª {uploaded_file.name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω!")

# –°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
if st.session_state.uploaded_files:
    st.subheader("üóÇ –°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
    
    for file_name, file_data in st.session_state.uploaded_files.items():
        cols = st.columns([4, 2, 2])
        
        with cols[0]:
            st.markdown(f"**–§–∞–π–ª:** `{file_name}`")
            st.markdown(f"**–°—Ç–∞—Ç—É—Å:** `{file_data['status']}`")
        
        with cols[2]:
            if st.button(f"–û—Ç–ø—Ä–∞–≤–∏—Ç—å {file_name}", key=f"send_{file_name}"):
                if file_data["df"] is not None:
                    with st.spinner("–û—Ç–ø—Ä–∞–≤–∫–∞..."):
                        success = send_to_kafka(
                            file_data["df"],
                            KAFKA_CONFIG["topic"],
                            KAFKA_CONFIG["bootstrap_servers"]
                        )
                        if success:
                            st.session_state.uploaded_files[file_name]["status"] = "–û—Ç–ø—Ä–∞–≤–ª–µ–Ω"
                            st.rerun()
                else:
                    st.error("–§–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö")

st.divider()
st.header("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–æ—Ä–∏–Ω–≥–∞")

if st.button("–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"):
    try:
        conn = psycopg2.connect(**POSTGRES_CONFIG)
    except Exception as e:
        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ: {e}")
    else:
        frauds_df = fetch_recent_frauds(conn, limit=10)
        if frauds_df.empty:
            st.info("–ù–µ—Ç –∑–∞–ø–∏—Å–µ–π —Å fraud_flag = 1.")
        else:
            st.subheader("–ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 fraud-—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
            st.dataframe(frauds_df, use_container_width=True)

        scores_df = fetch_recent_scores(conn, limit=100)
        if scores_df.empty:
            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã.")
        else:
            st.subheader("–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Å–∫–æ—Ä–æ–≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 100 —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
            chart = alt.Chart(scores_df).mark_bar().encode(
                alt.X("score:Q", bin=alt.Bin(maxbins=20), title="Score"),
                alt.Y("count()", title="Count")
            )
            st.altair_chart(chart, use_container_width=True)

        conn.close()
